# -*- coding: utf-8 -*-
"""LEAD SCORING CASE STUDY.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v0JmoAQ764WONjiTH-sNHHyec4SDR9GP

# LEAD SCORING CASE STUDY

### Problem Statement :

X Education is an education company that offers online courses to industry professionals. On a daily basis, numerous professionals interested in the courses visit their website and browse through the available courses. The company promotes its courses on various websites and search engines like Google. When these visitors land on the website, they may explore the courses, fill out a form to express their interest, or watch instructional videos. When a visitor fills out a form and provides their email address or phone number, they are classified as a lead. Additionally, the company also receives leads through referrals from past customers. Once these leads are acquired, the sales team initiates contact through calls, emails, and other means. Through this process, some leads are converted into paying customers while the majority do not convert. The typical lead conversion rate at X Education is around 30%. There is a large number of leads generated at the initial stage (top), but only a few of them end up becoming paying customers at the bottom. In the middle stage, it is crucial to nurture the potential leads effectively by educating them about the product and maintaining constant communication to increase the lead conversion rate. X Education has appointed you to help them identify the most promising leads, i.e., the leads with the highest likelihood of converting into paying customers. The company requires you to build a model that assigns a lead score to each lead, where leads with higher scores have a higher chance of conversion, and leads with lower scores have a lower chance of conversion. The CEO has set a target lead conversion rate of approximately 80%.

### Business Goal :

- Build a logistic regression model to assign a lead score ranging from 0 to 100 to each lead, allowing the company to effectively target potential leads. A higher score indicates a "hot" lead, meaning it is highly likely to convert, while a lower score suggests a "cold" lead with a lower chance of conversion.

- In addition, we need to address other potential challenges outlined in a separate document provided by the company. These challenges should be considered when developing the logistic regression model and will be included in the final presentation (PPT) along with recommendations. Please ensure that the document is filled based on the logistic regression model obtained in the initial step.

##  IMPORTING DATA
"""

# Supress Warnings

import warnings
warnings.filterwarnings('ignore')

# Commented out IPython magic to ensure Python compatibility.
# Importing Numpy vaf Pandas

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

# Importing dataset to csv

lead_df=pd.read_csv("Leads.csv")
lead_df.head()

"""## INSPECTING THE DATAFRAME"""

# Check the dimension of the dataframe

lead_df.shape

# Look at the statistics of dataframe

lead_df.describe()

lead_df.info()

"""## DATA CLEANING AND PREPARATION

### Converting values Yes/No to 1/0's in columns as taken into consideration

Encoding the variables with yes/no labels
"""

for feature in ['Do Not Email', 'Do Not Call', 'Search', 'Magazine', 'Newspaper Article','X Education Forums','Newspaper'
            ,'Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses'
            ,'Update me on Supply Chain Content','Get updates on DM Content','I agree to pay the amount through cheque'
           ,'A free copy of Mastering The Interview']:

     lead_df[feature] = lead_df[feature].apply(lambda x:1 if x=='Yes' else 0)
lead_df.head()

"""### Converting categorical to NaNs"""

# Converting all the selects to NaN as the user didnt select any option from the list and "Select" is as good as "NaN"

lead_df = lead_df.replace('Select', np.nan)

lead_df.info()

"""### MISSING VALUE HANDLING"""

# Check the ratio of missing values in each column

(lead_df.isnull().sum() / lead_df.shape[0] * 100).sort_values(ascending=False)

"""#### We drop columns with missing values greater than or equal to 70% to ensure data quality and avoid biased or unreliable analysis. Removing these columns improves the accuracy and reliability of our models and insights, and allows us to focus on variables with more complete information."""

#Dropping columns having more than 70% null values

lead_df = lead_df.drop(lead_df.loc[:,list(round(lead_df.isnull().sum()/lead_df.shape[0], 2)>0.70)].columns, 1)

# As the Lead Quality depends upons the intuition of the employee, it will be safer to update the NaN to "Not Sure"

lead_df['Lead Quality'] = lead_df['Lead Quality'].replace(np.nan, 'Not Sure')

"""#### Dropping those columns in which values are varying such as ('Asymmetrique Activity Index', 'Asymmetrique Activity Score', 'Asymmetrique Profile Index', 'Asymmetrique Profile Score').  Considering this, it may not provide significant variation or contribute much to lead classification. Hence, dropping this column could be considered to simplify the analysis and focus on other key factors."""

lead_df = lead_df.drop(['Asymmetrique Activity Index','Asymmetrique Activity Score','Asymmetrique Profile Index','Asymmetrique Profile Score'],1)

lead_df.select_dtypes(include='object').info()

for col in lead_df.iloc[:,1:].select_dtypes(include='object').columns:
    print(col)
    print("_________________________________________________________________________________________________")
    print(lead_df[col].value_counts(normalize= True))
    print("_________________________________________________________________________________________________")

"""#### Treating rest of the columns where values are missing"""

# We can impute the MUMBAI into all the NULLs as most of the values belong to MUMBAI

lead_df['City'] = lead_df['City'].replace(np.nan, 'Mumbai')

# Since there is no significant difference among top 3 specialisation , hence it will be safer to impute NaN with Others

lead_df['Specialization'] = lead_df['Specialization'].replace(np.nan, 'Other_Specialization')

# More than 85% data is of "Unemployed" and hence it is safer to impute NULLS with this value

lead_df['What is your current occupation'] = lead_df['What is your current occupation'].replace(np.nan, 'Unemployed')

# More than 99% data is of "Better Career Prospects" and hence it is safer to impute NULLS with this value

lead_df['What matters most to you in choosing a course'] = lead_df['What matters most to you in choosing a course'].replace(np.nan, 'Better Career Prospects')

# For Tags column, more than 30% data is for "Will revert after reading the email" and hence we can impute NULLS with Will revert after reading the email

lead_df['Tags'] = lead_df['Tags'].replace(np.nan, 'Will revert after reading the email')

# More than 95% data is of "India" and hence it is safer to impute NULLS with this value

lead_df['Country'] = lead_df['Country'].replace(np.nan, 'India')

# Check the ratio of missing values in each column again

(lead_df.isnull().sum() / lead_df.shape[0] * 100).sort_values(ascending=False)

# Check Lead Source column which 0.39% missing value

lead_df['Lead Source'].value_counts(normalize=True)

# Impute the 'google' in Lead Source by 'Google'

lead_df['Lead Source'] = lead_df['Lead Source'].replace('google', 'Google')
lead_df['Lead Source'].value_counts(normalize=True)

# Remaining NULL values are less than 2% and hence these rows can be directly dropped

lead_df.dropna(inplace=True)

lead_df.isnull().sum()

"""### Cleaning data

#### The columns "Magazine," "Receive More Updates About Our Courses," "Update me on Supply Chain Content," "Get updates on DM Content," and "I agree to pay the amount through cheque" have only one value ("No") in all the rows. Hence, these columns can be dropped as they do not provide any meaningful information or variability for analysis.
"""

lead_df= lead_df.drop(['Magazine', 'Receive More Updates About Our Courses', 'Update me on Supply Chain Content','Get updates on DM Content', 'I agree to pay the amount through cheque'],axis=1)

lead_df.shape

lead_df.head()

"""### Handle Outliers"""

# Check the outliers in all the numeric columns

plt.figure(figsize=(10, 3))
orange_color = sns.color_palette('Oranges')[2]

plt.subplot(1, 3, 1)
sns.boxplot(x='TotalVisits', data=lead_df, color=orange_color)
plt.title('TotalVisits')
plt.xlabel('')

plt.subplot(1, 3, 2)
sns.boxplot(x='Total Time Spent on Website', data=lead_df, color=orange_color)
plt.title('Total Time Spent on Website')
plt.xlabel('')

plt.subplot(1, 3, 3)
sns.boxplot(x='Page Views Per Visit', data=lead_df, color=orange_color)
plt.title('Page Views Per Visit')
plt.xlabel('')

plt.tight_layout()
plt.show()

# Checking outliers at 25%,50%,75%,90%,95% and above

lead_df.describe(percentiles=[.25,.5,.75,.90,.95,.99])

# Remove outliers by excluding data points at or above the 99%

lead_df = lead_df[lead_df['TotalVisits'] <= 17]
lead_df = lead_df[lead_df['Page Views Per Visit'] <= 9]

# Check the outliers in all the numeric columns again

plt.figure(figsize=(10, 3))
orange_color = sns.color_palette('Oranges')[2]

plt.subplot(1, 3, 1)
sns.boxplot(x='TotalVisits', data=lead_df, color=orange_color)
plt.title('TotalVisits')
plt.xlabel('')

plt.subplot(1, 3, 2)
sns.boxplot(x='Total Time Spent on Website', data=lead_df, color=orange_color)
plt.title('Total Time Spent on Website')
plt.xlabel('')

plt.subplot(1, 3, 3)
sns.boxplot(x='Page Views Per Visit', data=lead_df, color=orange_color)
plt.title('Page Views Per Visit')
plt.xlabel('')

plt.tight_layout()
plt.show()

"""## EXPLORATORY DATA ANALYSIS"""

# Target variable and see if we have any data imbalance or not

lead_df["Converted"].value_counts(normalize=True)

fig=plt.subplots(figsize=(6, 6))

sns.countplot(x="Lead Source", hue="Converted", data= lead_df)
plt.xticks(rotation='vertical')
plt.show()

fig=plt.subplots(figsize=(6, 6))
sns.countplot(x="Lead Origin", hue="Converted", data= lead_df)
plt.xticks(rotation='vertical')
plt.show()

"""OBSERVATION:

- API and Landing Page Submission has less conversion rate(~30%) but counts of the leads from them are considerable
- The count of leads from the Lead Add Form is pretty low but the conversion rate is very high

#### To improve the overall lead conversion rate, we need to focus on increasing the conversion rate of 'API' and 'Landing Page Submission' and also increasing the number of leads from 'Lead Add Form'
"""

# We can clearly observe that the count of leads from various sources are close to negligible and hence we can club them into "Others" source for better visualisation and analysis

lead_df['Lead Source'] = lead_df['Lead Source'].replace(['Pay per Click Ads', 'Press_Release','Social Media', 'WeLearn', 'bing', 'testone'], 'Other_Lead_Source')

# Plotting Lead Source again

sns.countplot(x="Lead Source", hue="Converted", data= lead_df)
plt.xticks(rotation='vertical')
plt.show()

"""OBSERVATION:

The count of leads from the Google and Direct Traffic is maximum The conversion rate of the leads from Google, Referal sites and Reference is maximum

#### To improve the overall lead conversion rate, we need to focus on increasing the conversion rate of 'Google', 'Olark Chat', 'Organic Search', 'Direct Traffic' and also increasing the number of leads from Referral sites, 'Reference' and 'Welingak Website'
"""

sns.boxplot(y = 'TotalVisits', x = 'Converted', data = lead_df)
 plt.show()

sns.boxplot(y = 'Total Time Spent on Website', x = 'Converted', data = lead_df)
 plt.show()

"""OBSERVATION:

- The median of both the conversion and non-conversion are same and hence nothing conclusive can be said using this information

- Users spending more time on the website are more likely to get converted

#### Websites can be made user friendly and more appealing so as to increase the time of the Users on websites
"""

# Plotting the Last Activity again

sns.countplot(x="Last Activity", hue="Converted", data= lead_df)
plt.xticks(rotation='vertical')
plt.show()

# Converting all the low count categories to the 'Others' category

lead_df['Last Activity'] = lead_df['Last Activity'].replace(['Had a Phone Conversation', 'View in browser link Clicked',
                                                       'Visited Booth in Tradeshow', 'Approached upfront',
                                                       'Resubscribed to emails','Email Received', 'Email Marked Spam'], 'Other Activity')

# Plotting the Last Activity again

sns.countplot(x="Last Activity", hue="Converted", data= lead_df)
plt.xticks( rotation='vertical')
plt.show()

"""OBSERVATION:

- The count of lst activity as "Email Opened" is max
- The conversion rate of SMS sent as last activity is maximum

#### We should focus on increasing the conversion rate of those having last activity as Email Opened by making a call to those leads and also try to increase the count of the ones having last activity as SMS sent
"""

sns.countplot(x="Specialization", hue="Converted", data= lead_df)
plt.xticks(rotation='vertical')
plt.show()

sns.countplot(x="What is your current occupation", hue="Converted", data= lead_df)
plt.xticks(rotation='vertical')
plt.show()

"""OBSERVATION:

Looking at above plot, no particular inference can be made for Specialization we can say that working professionals have high conversion rate Number of Unemployed leads are more than any other category

#### To increase overall conversion rate, we need to increase the number of Working Professional leads by reaching out to them through different social sites such as LinkedIn etc. and also on increasing the conversion rate of Unemployed leads
"""

lead_df[["Search","Newspaper Article","X Education Forums","Newspaper","Digital Advertisement",
         "Through Recommendations","A free copy of Mastering The Interview"]].describe()

sns.countplot(x="Lead Quality", hue="Converted", data= lead_df)
plt.xticks(rotation='vertical')
plt.show()

sns.countplot(x="Tags", hue="Converted", data= lead_df)
plt.xticks(rotation='vertical')
plt.show()

# Converting all low count categories to Others category
lead_df['Tags'] = lead_df['Tags'].replace(['In confusion whether part time or DLP', 'in touch with EINS','Diploma holder (Not Eligible)',
                                     'Approached upfront','Graduation in progress','number not provided', 'opp hangup','Still Thinking',
                                    'Lost to Others','Shall take in the next coming month','Lateral student','Interested in Next batch',
                                    'Recognition issue (DEC approval)','Want to take admission but has financial problems',
                                    'University not recognized'], 'Other_Tags')

# lets plot the Tags again

sns.countplot(x="Tags", hue="Converted", data= lead_df)
plt.xticks( rotation='vertical')
plt.show()

"""OBSERVATION:

'Will revert after reading the email' and 'Closed by Horizzon' have high conversion rate

SUMMARY:

- To improve the overall lead conversion rate, we need to focus on increasing the conversion rate of 'API' and 'Landing Page Submission' Lead Origins and also increasing the number of leads from 'Lead Add Form'

- To improve the overall lead conversion rate, we need to focus on increasing the conversion rate of 'Google', 'Olark Chat', 'Organic Search', 'Direct Traffic' and also increasing the number of leads from 'Reference' and 'Welingak Website'

- Websites can be made more appealing so as to increase the time of the Users on websites

- We should focus on increasing the conversion rate of those having last activity as Email Opened by making a call to those leads and also try to increase the count of the ones having last activity as SMS sent

- To increase overall conversion rate, we need to increase the number of Working Professional leads by reaching out to them through different social sites such as LinkedIn etc. and also on increasing the conversion rate of Unemployed leads

- We also observed that there are multiple columns which contains data of a single value only. As these columns do not contribute towards any inference, we can remove them from further analysis
"""

# Dropping unnecessary columns to focus on variables that have a more substantial impact on the analysis.

lead_df = lead_df.drop(['Lead Number','What matters most to you in choosing a course','Search','Newspaper Article','X Education Forums','Newspaper',
           'Digital Advertisement','Through Recommendations','A free copy of Mastering The Interview','Country','Do Not Call'],1)

lead_df.head()

"""### DUMMY VARIABLE CREATION"""

dummy = pd.get_dummies(lead_df[['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation',
                              'Tags','Lead Quality','City','Last Notable Activity']], drop_first=True)
dummy.head()

lead_df = pd.concat([lead_df, dummy], axis=1)
lead_df.head()

# Remove following columns :
lead_df = lead_df.drop(['Prospect ID', 'Lead Origin', 'Lead Source', 'Last Activity', 'Specialization', 'What is your current occupation','Tags','Lead Quality','City','Last Notable Activity'], axis=1)

lead_df.shape

lead_df.info()

"""## TEST-TRAIN SPLI"""

from sklearn.model_selection import train_test_split

# Putting feature variable to X
X = lead_df.drop(['Converted'], axis=1)


# Putting response variable to y
y = lead_df['Converted']

# Splitting the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)

"""### FEATURE SCALING"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])

X_train.head()

# Checking the Conversion Rate percentage

converted = (sum(lead_df['Converted'])/len(lead_df['Converted'].index))*100
converted

"""OBSERVATION:

The conversion rate has been successfully increased to 37.75%, indicating a significant improvement. This outcome suggests that the measures taken to identify potential leads and enhance the process have had a positive impact.

### LOOKING AT CORRELATIONS

#### Analyzes the correlation between variables, identifies highly correlated variables, and eliminates those that have a strong correlation with each other to avoid multicollinearity. It focuses on variables that exhibit a high correlation with the target variable 'Converted', aiding in the identification of crucial factors in the lead conversion process.
"""

# visualizing correlation by heatmap

plt.figure(figsize=(14, 10))
sns.heatmap(lead_df.corr(), cmap='viridis')
plt.show()

# columns pairs in order of highest absolute correlation

lead_df.corr().abs().unstack().sort_values(ascending=False).drop_duplicates().head(12)

# Dropping variables with high multi-collinearity
lead_df.drop(['Lead Source_Facebook', 'Last Notable Activity_Unsubscribed','Lead Source_Reference','Last Notable Activity_Email Opened','Last Activity_SMS Sent'], axis=1, inplace=True)

# Top features correlated with target variable
lead_df.corr()['Converted'].abs().sort_values(ascending=False).head(6)[1:]

"""## MODEL BUILDING

### Running the First Training Model
"""

import statsmodels.api as sm

# Logistic regression model
logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())
logm1.fit().summary()

"""### Feature Selection Using RFE"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()

# Import RFE

from sklearn.feature_selection import RFE

rfe = RFE(logreg, n_features_to_select=15)
rfe = rfe.fit(X_train, y_train)

rfe.support_

list(zip(X_train.columns, rfe.support_, rfe.ranking_))

# variables shortlisted by RFE
col1 = X_train.columns[rfe.support_]
col1

X_train.columns[~rfe.support_]

"""### MODEL 1"""

# Builds a logistic regression model (GLM)

X_train_sm = sm.add_constant(X_train[col1])
logm1 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm1.fit()
res.summary()

# Check variance inflation factor

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = pd.DataFrame()
vif['Features'] = X_train[col1].columns
vif['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col1].shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""#### The p-value associated with the 'Tags_switched off' column is found to be higher than the predetermined threshold. This indicates that the column's contribution to the model is not statistically significant. Therefore, remove the 'Tags_switched off' column from our model to improve its overall performance and accuracy."""

col2 = col1.drop('Tags_switched off',1)
col2

"""### MODEL 2"""

X_train_sm = sm.add_constant(X_train[col2])
logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()

# Check variance inflation factor

vif = pd.DataFrame()
vif['Features'] = X_train[col2].columns
vif['VIF'] = [variance_inflation_factor(X_train[col2].values, i) for i in range(X_train[col2].shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""#### Due to its high p-value exceeding the predefined threshold, the column 'Lead Source_Welingak Website' is considered statistically insignificant in our model. As a result, it is advisable to eliminate this column from our analysis to enhance the model's effectiveness and reliability."""

col3 = col2.drop('Lead Source_Welingak Website',1)
col3

"""### MODEL 3"""

X_train_sm = sm.add_constant(X_train[col3])
logm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm3.fit()
res.summary()

# Check variance inflation factor

vif = pd.DataFrame()
vif['Features'] = X_train[col3].columns
vif['VIF'] = [variance_inflation_factor(X_train[col3].values, i) for i in range(X_train[col3].shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""#### Since the p-value associated with the 'Tags_Ringing' column is higher than the defined threshold, it suggests that this variable lacks statistical significance in our model."""

col4 = col3.drop('Tags_Ringing',1)
col4

"""### MODEL 4

Builds a logistic regression model 4
"""

X_train_sm = sm.add_constant(X_train[col4])
logm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm3.fit()
res.summary()

"""#### Based on the analysis, it appears that Model 4 demonstrates stability and features significant p-values. Therefore, we have decided to proceed with this particular model for further analysis and exploration of the data.

### Creating a dataframe with the actual converted flag and the predicted Lead_Score probabilities
"""

# Getting the Predicted values on the train set

y_train_pred = res.predict(X_train_sm)
y_train_pred[:10]

y_train_pred = y_train_pred.values.reshape(-1)
y_train_pred[:10]

y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Lead_Score_Prob':y_train_pred})
y_train_pred_final['Prospect ID'] = y_train.index
y_train_pred_final.head()

"""OBSERVATION:

Based on these results, the model seems to be performing reasonably well in predicting the conversion likelihood for the provided prospects.

####  Classify the prospects as either likely to convert (1) or not likely to convert (0) based on their predicted conversion probabilities.
"""

# Creating new column 'Predicted' with value 1 if Lead_Score_Prob > 0.5 else 0

y_train_pred_final['predicted'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)
y_train_pred_final.head()

# Confusion matrix

from sklearn import metrics
from sklearn.metrics import confusion_matrix
confusion = confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )
print(confusion)

"""OBSERVATION:

Confusion matrix shows the counts of correctly and incorrectly predicted classes. In this case, there are 3,644 true negatives, 227 false negatives, 287 false positives, and 2,088 true positives.
"""

# Let's check the overall accuracy.

print(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))

"""OBSERVATION:

An accuracy score of 91.77% indicates that the model's predictions align with the actual values of the target variable ('Converted') in the training dataset with a high level of accuracy. It suggests that the model is performing well in correctly classifying the leads as converted or not converted.

### Checking VIFs
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs

vif = pd.DataFrame()
vif['Features'] = X_train[col4].columns
vif['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col4].shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

TP = confusion[1,1] # true positive
TN = confusion[0,0] # true negatives
FP = confusion[0,1] # false positives
FN = confusion[1,0] # false negatives

# Let's see the sensitivity of our logistic regression model

TP / float(TP+FN)

# Let us calculate specificity

TN / float(TN+FP)

# Calculate false postive rate - predicting non conversion when leads have converted

print(FP/ float(TN+FP))

# Calculate false postive rate - predicting non conversion when leads have converted

print(FP/ float(TN+FP))

# Positive predictive value

print (TP / float(TP+FP))

# Negative predictive value

print (TN / float(TN+ FN))

"""OBSERVATION:

- The features "Tags_Will revert after reading the email", "Lead Quality_Not Sure", and "Page Views Per Visit" have relatively high VIF values, indicating multicollinearity.
- The sensitivity of the logistic regression model is 0.8787, indicating that it correctly predicts 87.87% of converted leads.
- The specificity of the model is 0.9432, indicating that it correctly predicts 94.32% of not converted leads.
- The false positive rate is 0.0568, suggesting that around 5.68% of not converted leads are incorrectly predicted as converted.
- The positive predictive value is 0.9046, meaning that approximately 90.46% of predicted converted leads are actually converted.
- The negative predictive value is 0.9269, indicating that around 92.69% of predicted not converted leads are actually not converted.

### Plotting the ROC Curve
"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

def draw_roc(actual, probs):
    fpr, tpr, thresholds = roc_curve(actual, probs, drop_intermediate=False)
    auc_score = roc_auc_score(actual, probs)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score, color='blue')
    plt.plot([0, 1], [0, 1], 'k--', color='gray')
    plt.fill_between(fpr, tpr, color='lightblue', alpha=0.3)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.show()

    return None

from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve( y_train_pred_final.Converted, y_train_pred_final.Lead_Score_Prob, drop_intermediate = False )

draw_roc(y_train_pred_final.Converted, y_train_pred_final.Lead_Score_Prob)

"""OBSERVATION:

An ROC curve area of 0.96 indicates that the model performs very well in distinguishing between positive and negative classes. With a value of 0.96, the model is considered to have well classification ability with high accuracy in predicting positive and negative classes.

### Finding Optimal Cutoff Point
"""

# Create columns with different probability cutoffs

numbers = [float(x)/10 for x in range(10)]
for i in numbers:
    y_train_pred_final[i]= y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > i else 0)
y_train_pred_final.head()

# Calculate accuracy sensitivity and specificity for various probability cutoffs.

cutoff_df = pd.DataFrame(columns=['prob', 'accuracy', 'sensitivity', 'specificity'])
num = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

for i in num:
    cm1 = confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i])
    total1 = np.sum(cm1)
    accuracy = (cm1[0, 0] + cm1[1, 1]) / total1
    specificity = cm1[0, 0] / (cm1[0, 0] + cm1[0, 1])
    sensitivity = cm1[1, 1] / (cm1[1, 0] + cm1[1, 1])
    cutoff_df.loc[i] = [i, accuracy, sensitivity, specificity]

plt.figure(figsize=(8, 6))
plt.plot(cutoff_df['prob'], cutoff_df['accuracy'], marker='o', linestyle='-', color='blue', label='Accuracy')
plt.plot(cutoff_df['prob'], cutoff_df['sensitivity'], marker='o', linestyle='-', color='green', label='Sensitivity')
plt.plot(cutoff_df['prob'], cutoff_df['specificity'], marker='o', linestyle='-', color='red', label='Specificity')
plt.xlabel('Probability Cutoff')
plt.ylabel('Metrics Value')
plt.title('Accuracy, Sensitivity, and Specificity for Different Cutoffs')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)

cutoff_points = cutoff_df['prob']
plt.scatter(cutoff_points, cutoff_df['accuracy'], color='blue', marker='x')
plt.scatter(cutoff_points, cutoff_df['sensitivity'], color='green', marker='x')
plt.scatter(cutoff_points, cutoff_df['specificity'], color='red', marker='x')

plt.axvspan(0.2, 0.4, color='lightgray', alpha=0.3)

plt.show()

print(cutoff_df)

"""OBSERVATION:

A cutoff value of 0.3 yields an accuracy of 0.92, sensitivity of 0.92, and specificity of 0.91. This means the model predicts correctly 92% of the time, identifies 92% of the converted leads, and correctly identifies 91% of the non-converted leads. Overall, the model performs well with a balanced trade-off between correctly identifying converted leads and minimizing false positives.
"""

# Make the final prediction using 0.3 as the cut off

y_train_pred_final['final_predicted'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.3 else 0)
y_train_pred_final.head()

#Assigning lead score

y_train_pred_final['Lead_Score'] = y_train_pred_final.Lead_Score_Prob.map( lambda x: round(x*100))
y_train_pred_final.head()

# Check the overall accuracy.

metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)

confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )
confusion2

TP = confusion2[1,1] # true positive
TN = confusion2[0,0] # true negatives
FP = confusion2[0,1] # false positives
FN = confusion2[1,0] # false negatives

# Let's check the sensitivity

TP / float(TP+FN)

# Calculate specificity

TN / float(TN+FP)

# Calculate false postive rate

print(FP/ float(TN+FP))

# positive predictive value

print (TP / float(TP+FP))

# Negative predictive value

print (TN / float(TN+ FN))

#Calculating Precision

precision =round(TP/float(TP+FP),2)
precision

#Calculating Recall

recall = round(TP/float(TP+FN),2)
recall

# Calculating precision using precision_score function from sklearn

from sklearn.metrics import precision_score
precision_score(y_train_pred_final.Converted , y_train_pred_final.final_predicted)

from sklearn.metrics import recall_score
recall_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)

import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve

p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final['Lead_Score_Prob'])
plt.plot(thresholds, p[:-1], color='green', label="Precision")
plt.plot(thresholds, r[:-1], color='red', label="Recall")
plt.title('Precision-Recall Tradeoff')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.legend()
plt.grid(True)
plt.fill_between(thresholds, p[:-1], r[:-1], color='lightgray', alpha=0.3)
plt.show()

"""OBSERVATION:

The model has correctly predicted approximately 91.7% of the positive cases (converted). This suggests that the model is able to identify a high proportion of potential customers who have actually converted.

### Making predictions on the test set
"""

X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])

X_test = X_test[col4]

X_test.shape

X_test.head()

#add constant

X_test_sm = sm.add_constant(X_test)

#making predictions on test set

y_test_pred = res.predict(X_test_sm)

y_test_pred[:10]

# Converting y_pred to a dataframe which is an array

y_pred_1 = pd.DataFrame(y_test_pred)
y_pred_1.head()

# Converting y_test to dataframe

y_test_df = pd.DataFrame(y_test)

# Putting prospect ID to index

y_test_df['Prospect ID'] = y_test_df.index

# Removing index for both dataframes to append them side by side

y_pred_1.reset_index(drop=True, inplace=True)
y_test_df.reset_index(drop=True, inplace=True)

# Appending y_test_df and y_pred_1

y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)

y_pred_final.head()

# Renaming the column

y_pred_final= y_pred_final.rename(columns={ 0 : 'Lead_Score_Prob'})

# Rearranging the columns

y_pred_final = y_pred_final.reindex(['Prospect ID','Converted','Lead_Score_Prob'], axis=1)

# Adding Lead_Score column

y_pred_final['Lead_Score'] = round((y_pred_final['Lead_Score_Prob'] * 100),0)

y_pred_final['Lead_Score'] = y_pred_final['Lead_Score'].astype(int)

# Let's see the head of y_pred_final

y_pred_final.head()

y_pred_final['final_Predicted'] = y_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.3 else 0)

y_pred_final.head()

#classifying leads based on Lead score

y_pred_final['Lead_Type'] = y_pred_final.Lead_Score.map(lambda x: 'Hot Lead' if x >35 else 'Cold Lead')
y_pred_final.sort_values(by='Lead_Score', ascending = False)

# Let's check the overall accuracy.

metrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_Predicted)

confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_Predicted )
confusion2

TP = confusion2[1,1] # true positive
TN = confusion2[0,0] # true negatives
FP = confusion2[0,1] # false positives
FN = confusion2[1,0] # false negatives

# The sensitivity of our logistic regression model

TP / float(TP+FN)

# Calculate specificity

TN / float(TN+FP)

precision_score(y_pred_final.Converted , y_pred_final.final_Predicted)

recall_score(y_pred_final.Converted, y_pred_final.final_Predicted)

pd.options.display.float_format = '{:.2f}'.format
new_params = res.params[1:]
new_params

feature_importance = new_params
feature_importance = 100.0 * (feature_importance / feature_importance.max())
feature_importance

##Sorting the feature variables based on their relative coefficient values

sorted_idx = np.argsort(feature_importance,kind='quicksort',order='list of str')
sorted_idx

pos = np.arange(sorted_idx.shape[0]) + .5

featfig = plt.figure(figsize=(10, 6))
featax = featfig.add_subplot(1, 1, 1)

colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']
featax.barh(pos, feature_importance[sorted_idx], align='center', color=colors[:len(sorted_idx)], alpha=0.8)

featax.set_yticks(pos)
featax.set_yticklabels(np.array(X_train[col3].columns)[sorted_idx], fontsize=12)
featax.set_xlabel('Relative Feature Importance', fontsize=14)

featax.patch.set_facecolor('lightgray')
featax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)

plt.title('Feature Importance', fontsize=16)
plt.xlabel('Relative Importance', fontsize=14)
plt.ylabel('Features', fontsize=14)

plt.tight_layout()
plt.show()

"""OBSERVATION:

Based on the results, we have the following evaluation metrics for the model:

- Precision: 85.67%
- Recall: 92.05%

The coefficient values of the variables in the model, sorted by relative importance, are as follows:

- Tags_Lost to EINS: 100.00
- Tags_Closed by Horizzon: 98.27
- Tags_Busy: 55.64
- Tags_Will revert after reading the email: 55.24
- Total Time Spent on Website: 45.24
- Lead Origin_Lead Add Form: 30.68
- Last Notable Activity_SMS Sent: 26.20
- Page Views Per Visit: -19.70
- Do Not Email: -16.14
- Last Activity_Olark Chat Conversation: -14.94
- Lead Quality_Not Sure: -35.48
- Lead Quality_Worst: -28.62

SUMMARY

- Variables with high positive coefficient values such as Tags_Lost to EINS, Tags_Closed by Horizzon, Tags_Busy, and Tags_Will revert after reading the email have a positive impact on the lead conversion. This indicates that tags and related activities play an important role in generating high conversion potential.

- Total Time Spent on Website is also an important factor, indicating that customers who spend more time on the website are more likely to convert.

- Lead Origin_Lead Add Form and Last Notable Activity_SMS Sent also have a significant positive impact on conversion. This suggests that filling out lead add forms or sending the last SMS message can result in good conversion outcomes.

- Variables with negative coefficient values such as Page Views Per Visit, Do Not Email, Last Activity_Olark Chat Conversation, Lead Quality_Not Sure, and Lead Quality_Worst have a negative impact on conversion. This indicates that activities such as viewing multiple pages, not allowing email communication, engaging in Olark chat, and having uncertain or poor lead quality can reduce the conversion potential.

From the above results, we can optimize the lead conversion process by focusing on the factors that positively influence conversion and addressing the negative factors to improve overall conversion rates.
"""

